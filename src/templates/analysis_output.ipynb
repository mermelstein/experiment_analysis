{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: title_holder\n",
    "author: author_mcauthorson\n",
    "date: experiment_date\n",
    "format:\n",
    "  html:\n",
    "    echo: false\n",
    "    page-layout: full\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from BayesABTest import ab_test_model\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# load inputs\n",
    "experiment_file = open('experiment.json')\n",
    "experiment = json.load(experiment_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate binomial test data\n",
    "# THIS CELL SHOULD BE REMOVED IN PRODUCTION\n",
    "# data needs to be in a dataframe format where one column has the variant names, and one is a binomial outcome variable\n",
    "# expected column names are \"bucket\" for the variants. The binary outcome column name is up to you\n",
    "\n",
    "c_binom = np.random.binomial(size=100000, p=.1, n=1)\n",
    "t_binom = np.random.binomial(size=100000, p=.1025, n=1)\n",
    "\n",
    "c_binom_df = pd.DataFrame(c_binom, columns = ['outcomes'])\n",
    "t_binom_df = pd.DataFrame(t_binom, columns = ['outcomes'])\n",
    "\n",
    "c_binom_df['bucket'] = 'control'\n",
    "t_binom_df['bucket'] = 'test'\n",
    "\n",
    "variants_df = c_binom_df.append(t_binom_df)\n",
    "\n",
    "# load and set required inputs:\n",
    "\n",
    "confidence_interval = .9\n",
    "outcome_col = 'outcomes'\n",
    "control_variant = experiment['control_variant_name']\n",
    "\n",
    "# rename\n",
    "df = variants_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for production, use this to pull data\n",
    "# # ideally there is a table that holds all experimentation data with outcomes at the desired level of granularity (eg user)\n",
    "# # this would make it possible to templatize data sourcing for this kind of analysis\n",
    "# # another possibility would be to have multiple queries (stored elsewhere) that shape data as desired for the specific experiment or KPI being measured\n",
    "\n",
    "# confidence_interval = .9\n",
    "# outcome_col = 'outcomes' # The binary outcome column name is up to you, but needs to be specified\n",
    "# control_variant = experiment['control_variant_name']\n",
    "\n",
    "# df = pd.read_csv('experiment_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# initialize class with the data and some optional variables\n",
    "first_test = ab_test_model(df, \n",
    "                           metric=outcome_col, \n",
    "                           prior_info='uninformed', \n",
    "                           prior_function='beta', \n",
    "                           control_bucket_name=control_variant,\n",
    "                           confidence_level=1-confidence_interval,\n",
    "                           debug=True)\n",
    "\n",
    "# run public methods\n",
    "first_test.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TLDR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Observations: {len(df['bucket'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = df['bucket'].unique().tolist()\n",
    "variants.remove(control_variant)\n",
    "\n",
    "lower_bound = (1-confidence_interval)/2*100\n",
    "upper_bound = (confidence_interval + (1-confidence_interval)/2) * 100\n",
    "\n",
    "for var in variants:\n",
    "    lift = np.percentile(first_test.lift[var]['control'], 50)\n",
    "    likelihood_of_improvement = round(sum(i > 0 for i in first_test.lift[var][control_variant]) / len(first_test.lift[var][control_variant])*100,2)\n",
    "\n",
    "    if not np.percentile(first_test.lift[var][control_variant], lower_bound*5) <= 0 <= np.percentile(first_test.lift[var][control_variant], upper_bound) and likelihood_of_improvement >= 90.00:\n",
    "        print(f\"The {var} variant is better!\")\n",
    "        print(str(f\"Probable lift: {round(lift*100,2)}%\"))\n",
    "        print(f\"Likelihood of Improvement: {likelihood_of_improvement}%\")\n",
    "    else:\n",
    "        print(f\"No improvement for {var} variant, keep test running or call it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_test.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
